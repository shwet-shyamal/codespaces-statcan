{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks 2 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch modules\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Set torch to use cpu, because gpu is not powerful.\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cpu\")\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data'\n",
    "\n",
    "# this is where embedding data for training set will be stored\n",
    "converted_train_file = data_dir + '/encoded_train'\n",
    "# this is where embedding data for testing set will be stored\n",
    "converted_test_file = data_dir + '/encoded_test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download [CARS dataset](https://ai.stanford.edu/~jkrause/cars/car_dataset.html), consisting of roughly 8000 photos in training and testing datasets each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading training data...\n",
      "Downloading testing data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading training data...\")\n",
    "train_set = torchvision.datasets.StanfordCars(\n",
    "    root=data_dir,\n",
    "    split='train',\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor())\n",
    "\n",
    "print(\"Downloading testing data...\")\n",
    "test_set = torchvision.datasets.StanfordCars(\n",
    "    root=data_dir,\n",
    "    split='test',\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.no_grad at 0x7f12c8e3c8b0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download resnet 18 model, and tweak it for our purpose.\n",
    "\n",
    "encoder = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)#.to(device)\n",
    "encoder.fc = nn.Identity()\n",
    "\n",
    "# since the model is already trained, set to evaluation mode\n",
    "encoder.eval()\n",
    "# disable gradient computations to speed up inference.\n",
    "torch.no_grad() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create structures to hold embedding data\n",
    "\n",
    "converted_train_set = { i: {\n",
    "    'embedding': None,\n",
    "    'class_idx': -1,\n",
    "    'labelled': True} for i in range(len(train_set)) }\n",
    "\n",
    "converted_test_set = { i: {\n",
    "    'embedding': None,\n",
    "    'class_idx': -1,\n",
    "    'labelled': True} for i in range(len(test_set)) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100\n",
      "Iteration 200\n",
      "Iteration 300\n",
      "Iteration 400\n",
      "Iteration 500\n",
      "Iteration 600\n",
      "Iteration 700\n",
      "Iteration 800\n",
      "Iteration 900\n",
      "Iteration 1000\n",
      "Iteration 1100\n",
      "Iteration 1200\n",
      "Iteration 1300\n",
      "Iteration 1400\n",
      "Iteration 1500\n",
      "Iteration 1600\n",
      "Iteration 1700\n",
      "Iteration 1800\n",
      "Iteration 1900\n",
      "Iteration 2000\n",
      "Iteration 2100\n",
      "Iteration 2200\n",
      "Iteration 2300\n",
      "Iteration 2400\n",
      "Iteration 2500\n",
      "Iteration 2600\n",
      "Iteration 2700\n",
      "Iteration 2800\n",
      "Iteration 2900\n",
      "Iteration 3000\n",
      "Iteration 3100\n",
      "Iteration 3200\n",
      "Iteration 3300\n",
      "Iteration 3400\n",
      "Iteration 3500\n",
      "Iteration 3600\n",
      "Iteration 3700\n",
      "Iteration 3800\n",
      "Iteration 3900\n",
      "Iteration 4000\n",
      "Iteration 4100\n",
      "Iteration 4200\n",
      "Iteration 4300\n",
      "Iteration 4400\n",
      "Iteration 4500\n",
      "Iteration 4600\n",
      "Iteration 4700\n",
      "Iteration 4800\n",
      "Iteration 4900\n",
      "Iteration 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\x123\\xd1K\\xf4\\x81\\xe9\\x1b\\x19\\xdd\\x1cu']\n",
      "Bad pipe message: %s [b'\\nq\\x84\\xae \\xcf\\x9d\\x08j\\x1e&\\x1d-\\x8b\\x08 d\\x99!\\xf9\\xa5\\xaf\\x04\\xbb\\xa0']\n",
      "Bad pipe message: %s [b'\\x18/\\xdc\\x8bq\\xcc\\t\\xaab\\xf7\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x00+\\x00\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 D.ve\\xbb\\x0b\\xd1\\xf1\\xf5\\x9f\\xdd\\x03*\\xcd\\xb5N\\xb8O\\x95']\n",
      "Bad pipe message: %s [b\"\\x96\\xe9t\\xf5?}Q\\xbe\\xd3\\xa5\\xc0M\\xca\\x0bV\\xa1!n\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\"]\n",
      "Bad pipe message: %s [b'\\xcb\\xe0\\x05iz', b'\\xdd\\x12\\x8a\\x10\\xf2\\x05\\x0fz\\xb5\\x94\\xf5\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86']\n",
      "Bad pipe message: %s [b'\\x98\\xbf\\xee\\xb6\\xc8\\x04~:CC\\x9a\\xa9\\x80.t\\x06~\\x8a\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x00', b'7\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01']\n",
      "Bad pipe message: %s [b'\\x19\\xe4\\x86|\\xdb\\xc2\\x86\\x83\\xe6I\\xf1q\\x0e\\xcd\\xfbT&\\x14\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5100\n",
      "Iteration 5200\n",
      "Iteration 5300\n",
      "Iteration 5400\n",
      "Iteration 5500\n",
      "Iteration 5600\n",
      "Iteration 5700\n",
      "Iteration 5800\n",
      "Iteration 5900\n",
      "Iteration 6000\n",
      "Iteration 6100\n",
      "Iteration 6200\n",
      "Iteration 6300\n",
      "Iteration 6400\n",
      "Iteration 6500\n",
      "Iteration 6600\n",
      "Iteration 6700\n",
      "Iteration 6800\n",
      "Iteration 6900\n",
      "Iteration 7000\n",
      "Iteration 7100\n",
      "Iteration 7200\n",
      "Iteration 7300\n",
      "Iteration 7400\n",
      "Iteration 7500\n",
      "Iteration 7600\n",
      "Iteration 7700\n",
      "Iteration 7800\n",
      "Iteration 7900\n",
      "Iteration 8000\n",
      "Iteration 8100\n",
      "Iteration 100\n",
      "Iteration 200\n",
      "Iteration 300\n",
      "Iteration 400\n",
      "Iteration 500\n",
      "Iteration 600\n",
      "Iteration 700\n",
      "Iteration 800\n",
      "Iteration 900\n",
      "Iteration 1000\n",
      "Iteration 1100\n",
      "Iteration 1200\n",
      "Iteration 1300\n",
      "Iteration 1400\n",
      "Iteration 1500\n",
      "Iteration 1600\n",
      "Iteration 1700\n",
      "Iteration 1800\n",
      "Iteration 1900\n",
      "Iteration 2000\n",
      "Iteration 2100\n",
      "Iteration 2200\n",
      "Iteration 2300\n",
      "Iteration 2400\n",
      "Iteration 2500\n",
      "Iteration 2600\n",
      "Iteration 2700\n",
      "Iteration 2800\n",
      "Iteration 2900\n",
      "Iteration 3000\n",
      "Iteration 3100\n",
      "Iteration 3200\n",
      "Iteration 3300\n",
      "Iteration 3400\n",
      "Iteration 3500\n",
      "Iteration 3600\n",
      "Iteration 3700\n",
      "Iteration 3800\n",
      "Iteration 3900\n",
      "Iteration 4000\n",
      "Iteration 4100\n",
      "Iteration 4200\n",
      "Iteration 4300\n",
      "Iteration 4400\n",
      "Iteration 4500\n",
      "Iteration 4600\n",
      "Iteration 4700\n",
      "Iteration 4800\n",
      "Iteration 4900\n",
      "Iteration 5000\n",
      "Iteration 5100\n",
      "Iteration 5200\n",
      "Iteration 5300\n",
      "Iteration 5400\n",
      "Iteration 5500\n",
      "Iteration 5600\n",
      "Iteration 5700\n",
      "Iteration 5800\n",
      "Iteration 5900\n",
      "Iteration 6000\n",
      "Iteration 6100\n",
      "Iteration 6200\n",
      "Iteration 6300\n",
      "Iteration 6400\n",
      "Iteration 6500\n",
      "Iteration 6600\n",
      "Iteration 6700\n",
      "Iteration 6800\n",
      "Iteration 6900\n",
      "Iteration 7000\n",
      "Iteration 7100\n",
      "Iteration 7200\n",
      "Iteration 7300\n",
      "Iteration 7400\n",
      "Iteration 7500\n",
      "Iteration 7600\n",
      "Iteration 7700\n",
      "Iteration 7800\n",
      "Iteration 7900\n",
      "Iteration 8000\n"
     ]
    }
   ],
   "source": [
    "# Get embeddings using single processing.\n",
    "# Multi-processing on GitHub Computespace led to errors that I could not fix in time.\n",
    "\n",
    "def get_embeddings(source, destination):\n",
    "    i = 0\n",
    "    for image, label in source:\n",
    "        destination[i]['class_idx'] = label\n",
    "        destination[i]['embedding'] = encoder(image.unsqueeze(0)).squeeze().detach().numpy()\n",
    "        \n",
    "        i += 1\n",
    "        if i % 100 == 0:\n",
    "            print(\"Iteration\", i)\n",
    "\n",
    "get_embeddings(train_set, converted_train_set)\n",
    "torch.save(converted_train_set, converted_train_file)\n",
    "get_embeddings(test_set, converted_test_set)\n",
    "torch.save(converted_test_set, converted_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved embedding data\n",
    "\n",
    "train_data = torch.load(converted_train_file)\n",
    "test_data = torch.load(converted_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8144, 513)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# utility function to convert embedding data from dictionary to a dataframe\n",
    "def get_dataframe(dataset):\n",
    "    df = pd.DataFrame([ v['embedding'] for v in dataset.values() if v['labelled'] is True ])\n",
    "    df['label'] = [ v['class_idx'] for v in dataset.values() if v['labelled'] is True ]\n",
    "    return df\n",
    "\n",
    "df = get_dataframe(train_data)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def remove_label(data, proportion):\n",
    "    \"\"\" remove `proportion` fraction of data from `data`.\n",
    "    \n",
    "    Params:\n",
    "        data: a list with 2 elements - features and target respectively.\n",
    "        proportion: amount of data to remove\n",
    "    \n",
    "    Keeps at least 1 instance of all classes.\n",
    "    \"\"\"\n",
    "\n",
    "    X, y = data\n",
    "    y_unique = y.drop_duplicates()\n",
    "    n_samples = len(y)\n",
    "    n_unique = len(y_unique)\n",
    "\n",
    "    # partition the data into 2 parts:\n",
    "    # 1st part contains any one sample for each unique class.\n",
    "    # 2nd part contains rest of the samples\n",
    "\n",
    "    # partition 1\n",
    "    X_unique = X.loc[X.index.isin(y_unique.index)]\n",
    "    \n",
    "    # partition 2\n",
    "    X_rest = X.loc[~X.index.isin(y_unique.index)]\n",
    "    y_rest = y.loc[~y.index.isin(y_unique.index)]\n",
    "\n",
    "    # return all data from 1st partition + some data from 2nd partition such that\n",
    "    # total number of samples removed = correct proportion of full data\n",
    "    new_proportion = proportion * n_samples / (n_samples - n_unique)\n",
    "    if new_proportion > 1:\n",
    "        raise Exception(\"Asked to remove too many labels\")\n",
    "    elif new_proportion >= 0.999:\n",
    "        return X_unique, y_unique\n",
    "    else:\n",
    "        X_train, _, y_train, _ = train_test_split(\n",
    "            X_rest, y_rest, test_size=new_proportion, random_state=0, shuffle=True)\n",
    "    \n",
    "        return pd.concat([X_train, X_unique]), pd.concat([y_train, y_unique])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = remove_label([df.drop('label', axis='columns'), df['label']], 0.6)\n",
    "\n",
    "# keep unlabeled data for later use.\n",
    "df_unlabeled = df.loc[~df.index.isin(y.index)]\n",
    "X_unlabeled = df.drop('label', axis='columns').loc[~df.index.isin(y.index)]\n",
    "y_unlabeled = df['label'].loc[~df.index.isin(y.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "\n",
    "We use 3 models, trained on varying amounts of data.\n",
    "Points I will use to convince management to label more data:\n",
    "* We see that validation accuracy is highest for the 3rd model (the ones which uses the most data), even though the trend is not sharp.\n",
    "* Number of instances of least frequent class is too low (ranging from 2 to 7). Rule of thumb says that we should strive for at least 10 instances per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model using small data\n",
      "\tSample size = 1302\n",
      "\tNumber of classes = 196\n",
      "\tNumber of instances of least frequency class = 2\n",
      "\tValidation accuracy = 15.34%\n",
      "Training model using medium data\n",
      "\tSample size = 1953\n",
      "\tNumber of classes = 196\n",
      "\tNumber of instances of least frequency class = 4\n",
      "\tValidation accuracy = 15.03%\n",
      "Training model using large data\n",
      "\tSample size = 2605\n",
      "\tNumber of classes = 196\n",
      "\tNumber of instances of least frequency class = 7\n",
      "\tValidation accuracy = 20.55%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "import collections\n",
    "\n",
    "results = {\n",
    "    'small': {'keep_fraction' : 0.5},\n",
    "    'medium': {'keep_fraction' : 0.75},\n",
    "    'large': {'keep_fraction' : 1},\n",
    "}\n",
    "\n",
    "for k in results.keys():\n",
    "    print(f\"Training model using {k} data\")\n",
    "    if results[k]['keep_fraction'] < 1:\n",
    "        X_use, y_use = remove_label([X_train, y_train], 1 - results[k]['keep_fraction'])\n",
    "    else:\n",
    "        X_use, y_use = X_train, y_train\n",
    "\n",
    "    class_counts = sorted(collections.Counter(y_use).values())\n",
    "    least_frequent_class_count = class_counts[0]\n",
    "    print(f\"\\tSample size = {len(X_use)}\")\n",
    "    print(f\"\\tNumber of classes = {len(class_counts)}\")\n",
    "    print(f\"\\tNumber of instances of least frequency class = {least_frequent_class_count}\")\n",
    "    \n",
    "    model = SGDClassifier(\n",
    "        n_jobs=-1,\n",
    "        random_state=0,\n",
    "        # increase maximum number of allowable iterations if data is fewer,\n",
    "        # so that maximum amount of data to be learnt over remains same for different\n",
    "        # sample sizes.\n",
    "        max_iter=1000 / results[k]['keep_fraction'],\n",
    "        loss='log_loss')\n",
    "    \n",
    "    model.fit(X_use, y_use)\n",
    "    results[k]['model'] = model\n",
    "    results[k]['accuracy'] = model.score(X_validate, y_validate)\n",
    "    \n",
    "    print(f\"\\tValidation accuracy = {results[k]['accuracy'] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, fit the model on both training and validation data,\n",
    "# i.e. we use the full 40% of the training data.\n",
    "# This will likely give us the best possible model seen yet.\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "# get prediction probabilities on unlabeled training data (i.e. the remaining 60% data)\n",
    "probas = model.predict_proba(X_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8144 3257 5293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12640/1618629232.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_unlabeled['entropy'] = pd.Series([entropy(proba) for proba in probas], index=df_unlabeled.index)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "import math\n",
    "\n",
    "# add a new entropy column\n",
    "df_unlabeled['entropy'] = pd.Series([entropy(proba) for proba in probas], index=df_unlabeled.index)\n",
    "\n",
    "# pick number of rows = 25% of training dataset, with highest entropies\n",
    "df_2 = df_unlabeled.nlargest(math.floor(0.25 * len(df.index)), ['entropy'])\n",
    "\n",
    "# create a bigger labled training dataset = original labeled data + newly labeled data\n",
    "X_big = pd.concat([X, df_2.drop(['label', 'entropy'], axis='columns')])\n",
    "y_big = pd.concat([y, df_2['label']])\n",
    "\n",
    "# check the number of samples, to verify that\n",
    "# small training data = 40% of size of full training data\n",
    "# big training data = 65% of size of full training data\n",
    "print(len(df), len(X), len(X_big))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model on traind on 40% data = 42.33%\n",
      "Accuracy of model on traind on 65% data = 66.09%\n"
     ]
    }
   ],
   "source": [
    "# Now, finally, use the test data set (which we haven't touched at all so far)\n",
    "# to compare the 2 models\n",
    "df_test = get_dataframe(test_data)\n",
    "X_test = df.drop('label', axis='columns')\n",
    "y_test = df['label']\n",
    "\n",
    "print(f\"Accuracy of model on traind on 40% data = {model.score(X_test, y_test)* 100:.2f}%\")\n",
    "\n",
    "model.fit(X_big, y_big)\n",
    "print(f\"Accuracy of model on traind on 65% data = {model.score(X_test, y_test)* 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
