{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download the dataset and convert it into embeddings\n",
    "\n",
    "\n",
    "We'll use the [CARS dataset](https://ai.stanford.edu/~jkrause/cars/car_dataset.html) of ~8000 photos to build our image classifier. Get started by downloading the dataset with `torchvision` and previewing a handful of images from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data'\n",
    "converted_train_file = data_dir + '/encoded_train'\n",
    "converted_test_file = data_dir + '/encoded_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading training data...\n",
      "Downloading testing data...\n"
     ]
    }
   ],
   "source": [
    "# Download the CARS dataset to ./data\n",
    "print(\"Downloading training data...\")\n",
    "train_set = torchvision.datasets.StanfordCars(\n",
    "    root=data_dir,\n",
    "    split='train',\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor())\n",
    "\n",
    "print(\"Downloading testing data...\")\n",
    "test_set = torchvision.datasets.StanfordCars(\n",
    "    root=data_dir,\n",
    "    split='test',\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.no_grad at 0x7fe4a0486080>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)#.to(device)\n",
    "encoder.fc = nn.Identity()\n",
    "encoder.eval()\n",
    "torch.no_grad() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_train_set = { i: {\n",
    "    'embedding': None,\n",
    "    'class_idx': -1,\n",
    "    'labelled': True} for i in range(len(train_set)) }\n",
    "\n",
    "converted_test_set = { i: {\n",
    "    'embedding': None,\n",
    "    'class_idx': -1,\n",
    "    'labelled': True} for i in range(len(test_set)) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100\n",
      "Iteration 200\n",
      "Iteration 300\n",
      "Iteration 400\n",
      "Iteration 500\n",
      "Iteration 600\n",
      "Iteration 700\n",
      "Iteration 800\n",
      "Iteration 900\n",
      "Iteration 1000\n",
      "Iteration 1100\n",
      "Iteration 1200\n",
      "Iteration 1300\n",
      "Iteration 1400\n",
      "Iteration 1500\n",
      "Iteration 1600\n",
      "Iteration 1700\n",
      "Iteration 1800\n",
      "Iteration 1900\n",
      "Iteration 2000\n",
      "Iteration 2100\n",
      "Iteration 2200\n",
      "Iteration 2300\n",
      "Iteration 2400\n",
      "Iteration 2500\n",
      "Iteration 2600\n",
      "Iteration 2700\n",
      "Iteration 2800\n",
      "Iteration 2900\n",
      "Iteration 3000\n",
      "Iteration 3100\n",
      "Iteration 3200\n",
      "Iteration 3300\n",
      "Iteration 3400\n",
      "Iteration 3500\n",
      "Iteration 3600\n",
      "Iteration 3700\n",
      "Iteration 3800\n",
      "Iteration 3900\n",
      "Iteration 4000\n",
      "Iteration 4100\n",
      "Iteration 4200\n",
      "Iteration 4300\n",
      "Iteration 4400\n",
      "Iteration 4500\n",
      "Iteration 4600\n",
      "Iteration 4700\n",
      "Iteration 4800\n",
      "Iteration 4900\n",
      "Iteration 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\x123\\xd1K\\xf4\\x81\\xe9\\x1b\\x19\\xdd\\x1cu']\n",
      "Bad pipe message: %s [b'\\nq\\x84\\xae \\xcf\\x9d\\x08j\\x1e&\\x1d-\\x8b\\x08 d\\x99!\\xf9\\xa5\\xaf\\x04\\xbb\\xa0']\n",
      "Bad pipe message: %s [b'\\x18/\\xdc\\x8bq\\xcc\\t\\xaab\\xf7\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x00+\\x00\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 D.ve\\xbb\\x0b\\xd1\\xf1\\xf5\\x9f\\xdd\\x03*\\xcd\\xb5N\\xb8O\\x95']\n",
      "Bad pipe message: %s [b\"\\x96\\xe9t\\xf5?}Q\\xbe\\xd3\\xa5\\xc0M\\xca\\x0bV\\xa1!n\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\"]\n",
      "Bad pipe message: %s [b'\\xcb\\xe0\\x05iz', b'\\xdd\\x12\\x8a\\x10\\xf2\\x05\\x0fz\\xb5\\x94\\xf5\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86']\n",
      "Bad pipe message: %s [b'\\x98\\xbf\\xee\\xb6\\xc8\\x04~:CC\\x9a\\xa9\\x80.t\\x06~\\x8a\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x00', b'7\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01']\n",
      "Bad pipe message: %s [b'\\x19\\xe4\\x86|\\xdb\\xc2\\x86\\x83\\xe6I\\xf1q\\x0e\\xcd\\xfbT&\\x14\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5100\n",
      "Iteration 5200\n",
      "Iteration 5300\n",
      "Iteration 5400\n",
      "Iteration 5500\n",
      "Iteration 5600\n",
      "Iteration 5700\n",
      "Iteration 5800\n",
      "Iteration 5900\n",
      "Iteration 6000\n",
      "Iteration 6100\n",
      "Iteration 6200\n",
      "Iteration 6300\n",
      "Iteration 6400\n",
      "Iteration 6500\n",
      "Iteration 6600\n",
      "Iteration 6700\n",
      "Iteration 6800\n",
      "Iteration 6900\n",
      "Iteration 7000\n",
      "Iteration 7100\n",
      "Iteration 7200\n",
      "Iteration 7300\n",
      "Iteration 7400\n",
      "Iteration 7500\n",
      "Iteration 7600\n",
      "Iteration 7700\n",
      "Iteration 7800\n",
      "Iteration 7900\n",
      "Iteration 8000\n",
      "Iteration 8100\n",
      "Iteration 100\n",
      "Iteration 200\n",
      "Iteration 300\n",
      "Iteration 400\n",
      "Iteration 500\n",
      "Iteration 600\n",
      "Iteration 700\n",
      "Iteration 800\n",
      "Iteration 900\n",
      "Iteration 1000\n",
      "Iteration 1100\n",
      "Iteration 1200\n",
      "Iteration 1300\n",
      "Iteration 1400\n",
      "Iteration 1500\n",
      "Iteration 1600\n",
      "Iteration 1700\n",
      "Iteration 1800\n",
      "Iteration 1900\n",
      "Iteration 2000\n",
      "Iteration 2100\n",
      "Iteration 2200\n",
      "Iteration 2300\n",
      "Iteration 2400\n",
      "Iteration 2500\n",
      "Iteration 2600\n",
      "Iteration 2700\n",
      "Iteration 2800\n",
      "Iteration 2900\n",
      "Iteration 3000\n",
      "Iteration 3100\n",
      "Iteration 3200\n",
      "Iteration 3300\n",
      "Iteration 3400\n",
      "Iteration 3500\n",
      "Iteration 3600\n",
      "Iteration 3700\n",
      "Iteration 3800\n",
      "Iteration 3900\n",
      "Iteration 4000\n",
      "Iteration 4100\n",
      "Iteration 4200\n",
      "Iteration 4300\n",
      "Iteration 4400\n",
      "Iteration 4500\n",
      "Iteration 4600\n",
      "Iteration 4700\n",
      "Iteration 4800\n",
      "Iteration 4900\n",
      "Iteration 5000\n",
      "Iteration 5100\n",
      "Iteration 5200\n",
      "Iteration 5300\n",
      "Iteration 5400\n",
      "Iteration 5500\n",
      "Iteration 5600\n",
      "Iteration 5700\n",
      "Iteration 5800\n",
      "Iteration 5900\n",
      "Iteration 6000\n",
      "Iteration 6100\n",
      "Iteration 6200\n",
      "Iteration 6300\n",
      "Iteration 6400\n",
      "Iteration 6500\n",
      "Iteration 6600\n",
      "Iteration 6700\n",
      "Iteration 6800\n",
      "Iteration 6900\n",
      "Iteration 7000\n",
      "Iteration 7100\n",
      "Iteration 7200\n",
      "Iteration 7300\n",
      "Iteration 7400\n",
      "Iteration 7500\n",
      "Iteration 7600\n",
      "Iteration 7700\n",
      "Iteration 7800\n",
      "Iteration 7900\n",
      "Iteration 8000\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings(source, destination):\n",
    "    i = 0\n",
    "    for image, label in source:\n",
    "        destination[i]['class_idx'] = label\n",
    "        destination[i]['embedding'] = encoder(image.unsqueeze(0)).squeeze().detach().numpy()\n",
    "        \n",
    "        i += 1\n",
    "        if i % 100 == 0:\n",
    "            print(\"Iteration\", i)\n",
    "\n",
    "get_embeddings(train_set, converted_train_set)\n",
    "torch.save(converted_train_set, converted_train_file)\n",
    "get_embeddings(test_set, converted_test_set)\n",
    "torch.save(converted_test_set, converted_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load(converted_train_file)\n",
    "test_data = torch.load(converted_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: train classifiers\n",
    "\n",
    "Now that we have our dataset, we need to set up a neural network for PyTorch. Our neural network will transform an image into a description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# convert dictionary to table\n",
    "def get_dataframe(dataset):\n",
    "    df = pd.DataFrame([ v['embedding'] for v in dataset.values() if v['labelled'] is True ])\n",
    "    df['label'] = [ v['class_idx'] for v in dataset.values() if v['labelled'] is True ]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def remove_label(data, proportion):\n",
    "    X, y = data\n",
    "\n",
    "    unique_classes = set(y)\n",
    "    unique_indices = []\n",
    "    other_indices = []\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i] in unique_classes:\n",
    "            unique_indices.append(i)\n",
    "            unique_classes.remove(y[i])\n",
    "        else:\n",
    "            other_indices.append(i)\n",
    "\n",
    "    num_unique = len(unique_indices)\n",
    "    X_unique = np.take(X, unique_indices, axis=0)\n",
    "    X_rest = np.take(X, other_indices, axis=0)\n",
    "    y_unique = np.take(y, unique_indices)\n",
    "    y_rest = np.take(y, other_indices)\n",
    "\n",
    "    new_proportion = proportion * len(y) / (len(y) - num_unique)\n",
    "    if new_proportion > 1:\n",
    "        raise Exception(\"Asked to remove too many labels\")\n",
    "\n",
    "    X_train, _, y_train, _ = train_test_split(\n",
    "        X_rest, y_rest, test_size=new_proportion, random_state=0, shuffle=True)\n",
    "    \n",
    "    return pd.concat([X_train, X_unique]) , pd.concat([y_train, y_unique])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8144, 513)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_dataframe(train_data)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = remove_label([df.drop('label', axis='columns'), df['label']], 0.6)\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3257, 512)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2263\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2273\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcollections\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m X_small, y_small \u001b[39m=\u001b[39m remove_label([X_train, y_train], \u001b[39m0.5\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m least_frequent_class_count \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(collections\u001b[39m.\u001b[39mCounter(y_small)\u001b[39m.\u001b[39mvalues())[\u001b[39m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of instances of least frequency class = \u001b[39m\u001b[39m{\u001b[39;00mleast_frequent_class_count\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 11\u001b[0m, in \u001b[0;36mremove_label\u001b[0;34m(data, proportion)\u001b[0m\n\u001b[1;32m      8\u001b[0m other_indices \u001b[39m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(y)):\n\u001b[0;32m---> 11\u001b[0m     \u001b[39mif\u001b[39;00m y[i] \u001b[39min\u001b[39;00m unique_classes:\n\u001b[1;32m     12\u001b[0m         unique_indices\u001b[39m.\u001b[39mappend(i)\n\u001b[1;32m     13\u001b[0m         unique_classes\u001b[39m.\u001b[39mremove(y[i])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m    980\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 981\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    984\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   1088\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   1090\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "X_small, y_small = remove_label([X_train, y_train], 0.5)\n",
    "least_frequent_class_count = sorted(collections.Counter(y_small).values())[0]\n",
    "print(f\"Number of instances of least frequency class = {least_frequent_class_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2263\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2273\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 16\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m#model = SGDClassifier(\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m#    n_jobs=-1,\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m#    random_state=0,\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m#    max_iter=1000 / keep_fraction)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m param_grid \u001b[39m=\u001b[39m {\n\u001b[1;32m     12\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39mhinge\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlog_loss\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     13\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39malpha\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m1e-10\u001b[39m, \u001b[39m1e-9\u001b[39m, \u001b[39m1e-8\u001b[39m, \u001b[39m1e-7\u001b[39m, \u001b[39m1e-6\u001b[39m, \u001b[39m1e-5\u001b[39m, \u001b[39m1e-4\u001b[39m, \u001b[39m1e-3\u001b[39m, \u001b[39m1e-2\u001b[39m, \u001b[39m1e-1\u001b[39m, \u001b[39m1\u001b[39m],\n\u001b[1;32m     14\u001b[0m }\n\u001b[0;32m---> 16\u001b[0m X_small, y_small \u001b[39m=\u001b[39m remove_label([X_train, y_train], \u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m keep_fraction)\n\u001b[1;32m     17\u001b[0m least_frequent_class_count \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(collections\u001b[39m.\u001b[39mCounter(y_small)\u001b[39m.\u001b[39mvalues())[\u001b[39m0\u001b[39m]\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of instances of least frequency class = \u001b[39m\u001b[39m{\u001b[39;00mleast_frequent_class_count\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m, in \u001b[0;36mremove_label\u001b[0;34m(data, proportion)\u001b[0m\n\u001b[1;32m      8\u001b[0m other_indices \u001b[39m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(y)):\n\u001b[0;32m---> 11\u001b[0m     \u001b[39mif\u001b[39;00m y[i] \u001b[39min\u001b[39;00m unique_classes:\n\u001b[1;32m     12\u001b[0m         unique_indices\u001b[39m.\u001b[39mappend(i)\n\u001b[1;32m     13\u001b[0m         unique_classes\u001b[39m.\u001b[39mremove(y[i])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m    980\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 981\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    984\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   1088\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   1090\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import collections\n",
    "\n",
    "keep_fraction = 0.5\n",
    "#model = SGDClassifier(\n",
    "#    n_jobs=-1,\n",
    "#    random_state=0,\n",
    "#    max_iter=1000 / keep_fraction)\n",
    "\n",
    "param_grid = {\n",
    "    \"loss\": ['hinge', 'log_loss'],\n",
    "    \"alpha\": [1e-10, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1],\n",
    "}\n",
    "\n",
    "X_small, y_small = remove_label([X_train, y_train], 1 - keep_fraction)\n",
    "least_frequent_class_count = sorted(collections.Counter(y_small).values())[0]\n",
    "print(f\"Number of instances of least frequency class = {least_frequent_class_count}\")\n",
    "#search = GridSearchCV(\n",
    "#    model,\n",
    "#    param_grid,\n",
    "#    n_jobs=-1,\n",
    "#    error_score='raise',\n",
    "#    cv=min(10, max(least_frequent_class_count, 3)))\n",
    "#search.fit(X_small, y_small)\n",
    "#model_small = search.best_estimator_\n",
    "\n",
    "#model_small.score(X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen hyper-parameters:\n",
      " {'alpha': 0.001, 'loss': 'log_loss'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Chosen hyper-parameters:\\n\", search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.147239263803681"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_fraction = 0.75\n",
    "model = SGDClassifier(\n",
    "    n_jobs=-1,\n",
    "    random_state=0,\n",
    "    max_iter=1000 / keep_fraction)\n",
    "\n",
    "X_medium, y_medium = remove_label([X_train, y_train], 1 - keep_fraction)\n",
    "least_frequent_class_count = sorted(collections.Counter(y_medium).values())[0]\n",
    "search = GridSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    n_jobs=-1,\n",
    "    error_score='raise',\n",
    "    cv=min(10, max(least_frequent_class_count, 3)))\n",
    "search.fit(X_medium, y_medium)\n",
    "model_medium = search.best_estimator_\n",
    "model_medium.score(X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen hyper-parameters:\n",
      " {'alpha': 0.0001, 'loss': 'log_loss'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Chosen hyper-parameters:\\n\", search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1411042944785276"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SGDClassifier(\n",
    "    n_jobs=-1,\n",
    "    random_state=0,\n",
    "    max_iter=1000)\n",
    "\n",
    "X_large, y_large = X_train, y_train\n",
    "least_frequent_class_count = sorted(collections.Counter(y_large).values())[0]\n",
    "search = GridSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    n_jobs=-1,\n",
    "    error_score='raise',\n",
    "    cv=min(10, max(least_frequent_class_count, 3)))\n",
    "search.fit(X_large, y_large)\n",
    "model_large = search.best_estimator_\n",
    "model_large.score(X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen hyper-parameters:\n",
      " {'alpha': 0.0001, 'loss': 'log_loss'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Chosen hyper-parameters:\\n\", search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 43 48\n"
     ]
    }
   ],
   "source": [
    "print(model_small.n_iter_, model_medium.n_iter_, model_large.n_iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Train the network and save model\n",
    "\n",
    "PyTorch trains our network by adjusting its parameters and evaluating its performance against our labelled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 2: 100%|████████████████████████| 5000/5000 [00:20<00:00, 241.32it/s]\n",
      "Epoch 2 of 2: 100%|████████████████████████| 5000/5000 [00:20<00:00, 239.70it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 2\n",
    "print(\"Training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(tqdm(trainloader, desc=f\"Epoch {epoch + 1} of {EPOCHS}\", leave=True, ncols=80)):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Save our trained model\n",
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Test the trained model\n",
    "\n",
    "Let's test our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Evaluate model accuracy\n",
    "\n",
    "Let's conclude by evaluating our model's overall performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
